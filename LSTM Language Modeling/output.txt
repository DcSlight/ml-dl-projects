Loading file: /content/language_models/datasets/wikisimple.txt
Read 29742 lines from the file.
Processed 29741 valid sentences.
Vocabulary size: 10002 words.
max length per batch:  [11, 15, 16, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 37, 37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 39, 39, 39, 39, 39, 39, 40, 40, 40, 40, 40, 41, 41, 41, 41, 41, 42, 42, 42, 42, 43, 43, 43, 43, 44, 44, 44, 44, 45, 45, 45, 46, 46, 47, 47, 48, 48, 49, 49, 50, 50, 51, 52, 53, 54, 54, 56, 57, 58, 60, 62, 65, 71, 79, 170]
Loaded dataset: 23792 train, 2974 val, 2975 test

 Training LSTM_1L_Normal...
100%|██████████| 465/465 [00:18<00:00, 25.67it/s]
Epoch 1, Loss: 5.794163227081299
100%|██████████| 465/465 [00:15<00:00, 29.78it/s]
Epoch 2, Loss: 5.3999528884887695
100%|██████████| 465/465 [00:14<00:00, 31.97it/s]
Epoch 3, Loss: 5.182365894317627
LSTM_1L_Normal training complete. Model saved.
Calculating perplexity for LSTM_1L_Normal...
100%|██████████| 5000/5000 [07:37<00:00, 10.92it/s]
100%|██████████| 2974/2974 [04:42<00:00, 10.52it/s]
100%|██████████| 2975/2975 [04:49<00:00, 10.28it/s]
Perplexity for LSTM_1L_Normal: {'Train Perplexity': 231.01041789305853, 'Validation Perplexity': 234.832565671371, 'Test Perplexity': 234.6455897869118}

 Training LSTM_2L_Normal...
100%|██████████| 465/465 [00:20<00:00, 22.18it/s]
Epoch 1, Loss: 6.077306270599365
100%|██████████| 465/465 [00:15<00:00, 29.19it/s]
Epoch 2, Loss: 5.898080825805664
100%|██████████| 465/465 [00:15<00:00, 29.12it/s]
Epoch 3, Loss: 5.802323341369629
LSTM_2L_Normal training complete. Model saved.
Calculating perplexity for LSTM_2L_Normal...
100%|██████████| 5000/5000 [08:06<00:00, 10.28it/s]
100%|██████████| 2974/2974 [04:56<00:00, 10.03it/s]
100%|██████████| 2975/2975 [04:37<00:00, 10.74it/s]
Perplexity for LSTM_2L_Normal: {'Train Perplexity': 446.26599827359183, 'Validation Perplexity': 450.72638980276383, 'Test Perplexity': 447.51460300513037}

 Training LSTM_1L_Reversed...
100%|██████████| 465/465 [00:17<00:00, 26.43it/s]
Epoch 1, Loss: 5.861127853393555
100%|██████████| 465/465 [00:14<00:00, 31.66it/s]
Epoch 2, Loss: 5.500492095947266
100%|██████████| 465/465 [00:14<00:00, 32.19it/s]
Epoch 3, Loss: 5.274121284484863
LSTM_1L_Reversed training complete. Model saved.
Calculating perplexity for LSTM_1L_Reversed...
100%|██████████| 5000/5000 [07:23<00:00, 11.29it/s]
100%|██████████| 2974/2974 [04:22<00:00, 11.31it/s]
100%|██████████| 2975/2975 [04:28<00:00, 11.08it/s]
Perplexity for LSTM_1L_Reversed: {'Train Perplexity': 2460.489996846953, 'Validation Perplexity': 2491.994312724488, 'Test Perplexity': 2424.2114787170117}

 Training LSTM_2L_Reversed...
100%|██████████| 465/465 [00:20<00:00, 22.99it/s]
Epoch 1, Loss: 6.218125343322754
100%|██████████| 465/465 [00:15<00:00, 29.79it/s]
Epoch 2, Loss: 6.06163215637207
100%|██████████| 465/465 [00:15<00:00, 29.27it/s]
Epoch 3, Loss: 5.965739727020264
LSTM_2L_Reversed training complete. Model saved.
Calculating perplexity for LSTM_2L_Reversed...
100%|██████████| 5000/5000 [08:01<00:00, 10.38it/s]
100%|██████████| 2974/2974 [04:38<00:00, 10.69it/s]
100%|██████████| 2975/2975 [04:39<00:00, 10.65it/s]Perplexity for LSTM_2L_Reversed: {'Train Perplexity': 883.772862489231, 'Validation Perplexity': 892.7597794032154, 'Test Perplexity': 877.3102027534011}

Final Perplexity Results:

 LSTM_1L_Normal
Train Perplexity: 231.0104
Validation Perplexity: 234.8326
Test Perplexity: 234.6456

 LSTM_2L_Normal
Train Perplexity: 446.2660
Validation Perplexity: 450.7264
Test Perplexity: 447.5146

 LSTM_1L_Reversed
Train Perplexity: 2460.4900
Validation Perplexity: 2491.9943
Test Perplexity: 2424.2115

 LSTM_2L_Reversed
Train Perplexity: 883.7729
Validation Perplexity: 892.7598
Test Perplexity: 877.3102



Generating text for seed: 'I love'
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 251ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step
<ipython-input-5-6f16fd6f58d8>:28: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)
  tokens[i] = next_token
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step

Temperature = 0.1
Generated: I love 1873 Derbyshire movements Schoenberg meter
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 33ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 38ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step

Temperature = 1
Generated: I love Royal the . recently state
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 30ms/step
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step

Temperature = 10
Generated: I love post Grange ancestor Colonel Alonso


Model: "functional_4"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ input_layer_4 (InputLayer)           │ (None, None)                │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ embedding_4 (Embedding)              │ (None, None, 300)           │       3,000,600 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ lstm_6 (LSTM)                        │ (None, None, 256)           │         570,368 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ time_distributed_4 (TimeDistributed) │ (None, None, 10002)         │       2,570,514 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
 Total params: 18,424,448 (70.28 MB)
 Trainable params: 6,141,482 (23.43 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 12,282,966 (46.86 MB)
Model: "functional_6"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ input_layer_6 (InputLayer)           │ (None, None)                │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ embedding_6 (Embedding)              │ (None, None, 300)           │       3,000,600 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ lstm_9 (LSTM)                        │ (None, None, 256)           │         570,368 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ time_distributed_6 (TimeDistributed) │ (None, None, 10002)         │       2,570,514 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
 Total params: 18,424,448 (70.28 MB)
 Trainable params: 6,141,482 (23.43 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 12,282,966 (46.86 MB)
Model: "functional_5"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ input_layer_5 (InputLayer)           │ (None, None)                │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ embedding_5 (Embedding)              │ (None, None, 300)           │       3,000,600 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ lstm_7 (LSTM)                        │ (None, None, 256)           │         570,368 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ lstm_8 (LSTM)                        │ (None, None, 256)           │         525,312 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ time_distributed_5 (TimeDistributed) │ (None, None, 10002)         │       2,570,514 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
 Total params: 20,000,384 (76.30 MB)
 Trainable params: 6,666,794 (25.43 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 13,333,590 (50.86 MB)
Model: "functional_7"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ input_layer_7 (InputLayer)           │ (None, None)                │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ embedding_7 (Embedding)              │ (None, None, 300)           │       3,000,600 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ lstm_10 (LSTM)                       │ (None, None, 256)           │         570,368 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ lstm_11 (LSTM)                       │ (None, None, 256)           │         525,312 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ time_distributed_7 (TimeDistributed) │ (None, None, 10002)         │       2,570,514 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
 Total params: 20,000,384 (76.30 MB)
 Trainable params: 6,666,794 (25.43 MB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 13,333,590 (50.86 MB)

 The sentence: hello world 
Has probability of: 0.00000473

Enter a word: we
Predicted next word: allows

The sentence: I love 1873 Derbyshire movements Schoenberg meter 
Has probability of: 0.00000000
The sentence: I love Royal the . recently state 
Has probability of: 0.00000000
The sentence: I love post Grange ancestor Colonel Alonso 
Has probability of: 0.00000000
The sentence: i love cupcakes 
Has probability of: 0.00000000